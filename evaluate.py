import torch
from tqdm import tqdm
import os
import csv
import datetime


def evaluate_clean(model, data_loader, device, epoch=0, log_dir="evaluation_results"):
    correct = 0
    total = 0

    model.eval()

    # âœ… åˆ›å»ºæ¯è½®è®­ç»ƒçš„æ–‡ä»¶å¤¹ï¼Œé¿å…è¦†ç›–
    epoch_log_dir = os.path.join(log_dir, f"epoch_{epoch + 1}")
    os.makedirs(epoch_log_dir, exist_ok=True)

    # âœ… è®¾ç½®æ—¥å¿—æ–‡ä»¶å
    log_file = os.path.join(epoch_log_dir, "evaluation_progress_train.csv")

    # âœ… å®šä¹‰è¡¨å¤´å®½åº¦æ ¼å¼
    col_width = 10
    separator = f"|{'-' * col_width}|{'-' * col_width}|\n"
    header = f"|{'Batch'.center(col_width)}|{'Accuracy (%)'.center(col_width)}|\n"

    # âœ… å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆæ¸…ç©ºå†…å®¹
    if os.path.exists(log_file):
        open(log_file, 'w').close()

    # âœ… åˆå§‹åŒ– CSV æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´å’Œåˆ†éš”çº¿
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(header)
        f.write(separator)

    with torch.no_grad():
        # âœ… è®¾ç½® tqdm åªæ˜¾ç¤ºä¸€è¡ŒåŠ¨æ€è¿›åº¦æ¡
        progress_bar = tqdm(data_loader, desc="Evaluating", dynamic_ncols=True)

        # âœ… éå†æ•°æ®
        for batch_idx, batch in enumerate(progress_bar, start=1):
            images, input_ids, attention_mask, labels = batch

            # âœ… æ•°æ®æ¬åˆ° GPU / CPU
            images = images.to(device)
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)

            # âœ… æ¨¡å‹æ¨ç†
            outputs = model(
                pixel_values=images,
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # âœ… è·å–é¢„æµ‹ç»“æœ
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)
            predicted_answers = [model.config.id2label[pred.item()] for pred in preds]

            # âœ… è®¡ç®—æ­£ç¡®ç‡
            for pred_answer, true_answer in zip(predicted_answers, labels):
                if pred_answer.lower().strip() == true_answer.lower().strip():
                    correct += 1
                total += 1

            # âœ… æ¯500æ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡å‡†ç¡®ç‡
            if batch_idx % 500 == 0:
                current_accuracy = correct / total * 100
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write(f"|{str(batch_idx).center(col_width)}|{f'{current_accuracy:.2f}'.center(col_width)}|\n")
                    f.write(separator)  # âœ… ä¿å­˜å®Œå†åŠ åˆ†éš”çº¿
                print(f"ğŸ’¾ å·²ä¿å­˜ç¬¬ {batch_idx} æ‰¹æ¬¡çš„å‡†ç¡®ç‡: {current_accuracy:.2f}%")

            # âœ… æ¯æ¬¡è¿­ä»£å®æ—¶æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                "Correct": correct,
                "Total": total,
                "Acc": f"{(correct / total * 100):.2f}%"
            })

    # âœ… æœ€ç»ˆæ‰“å°æœ€ç»ˆçš„å‡†ç¡®ç‡
    final_accuracy = correct / total * 100
    print(f"âœ… æœ€ç»ˆæ¨¡å‹åœ¨å¹²å‡€æ•°æ®ä¸Šçš„å‡†ç¡®ç‡: {final_accuracy:.2f}%")

    # âœ… æœ€åä¿å­˜ä¸€æ¬¡æœ€ç»ˆå‡†ç¡®ç‡ï¼Œå¹¶åŠ ä¸Šæœ€ç»ˆåˆ†éš”çº¿
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"|{'Final'.center(col_width)}|{f'{final_accuracy:.2f}'.center(col_width)}|\n")
        f.write(f"|{'=' * col_width}|{'=' * col_width}|\n")  # âœ… æœ€ç»ˆç»“æœç”¨åŠ ç²—çº¿

    print(f"âœ… å®Œæˆï¼æœ€ç»ˆå‡†ç¡®ç‡å·²ä¿å­˜åˆ° {log_file}")
    return final_accuracy


def evaluate_backdoor(model, data_loader, device, target_label="wallet"):
    """è¯„ä¼°åé—¨æ”»å‡»æˆåŠŸç‡ï¼ˆå¸¦å®æ—¶æ—¥å¿—è®°å½•ï¼‰"""
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—ç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join("backdoor_logs", timestamp)
    os.makedirs(log_dir, exist_ok=True)

    # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = os.path.join(log_dir, "attack_progress.csv")

    # åˆå§‹åŒ–è®¡æ•°å™¨
    successful_attacks = 0
    total_samples = 0

    # åˆ›å»ºå¹¶åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    with open(log_file, "w") as f:
        f.write("timestamp,batch_num,success_count,total_samples,asr\n")  # CSVå¤´éƒ¨

    model.eval()
    with torch.no_grad():
        # å¸¦è¿›åº¦æ¡çš„è¿­ä»£å™¨
        progress_bar = tqdm(data_loader, desc="Backdoor Evaluation")

        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for batch_idx, batch in enumerate(progress_bar, 1):
            # æ•°æ®é¢„å¤„ç†
            images, input_ids, attention_mask, _ = batch
            images = images.to(device)
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)

            # æ¨¡å‹æ¨ç†
            outputs = model(
                pixel_values=images,
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # ç»“æœè§£æ
            preds = torch.argmax(outputs.logits, dim=1)
            predicted_answers = [model.config.id2label[p.item()] for p in preds]

            # ç»Ÿè®¡æ”»å‡»æˆåŠŸç‡
            batch_success = sum(
                1 for pred in predicted_answers
                if pred.lower().strip() == target_label.lower().strip()
            )
            successful_attacks += batch_success
            total_samples += len(predicted_answers)

            # å®æ—¶æ›´æ–°æ—¥å¿—ï¼ˆæ¯ä¸ªæ‰¹æ¬¡éƒ½è®°å½•ï¼‰
            current_time = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
            current_asr = successful_attacks / total_samples * 100 if total_samples > 0 else 0

            with open(log_file, "a") as f:
                f.write(f"{current_time},{batch_idx},{successful_attacks},{total_samples},{current_asr:.2f}\n")

            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            progress_bar.set_postfix({
                "ASR": f"{current_asr:.2f}%",
                "Success": successful_attacks,
                "Total": total_samples
            })

    # æœ€ç»ˆç»“æœå¤„ç†
    final_asr = successful_attacks / total_samples * 100 if total_samples > 0 else 0
    print(f"\nğŸ”¥ æœ€ç»ˆæ”»å‡»æˆåŠŸç‡: {final_asr:.2f}%")
    print(f"ğŸ“ å®Œæ•´æ—¥å¿—ä¿å­˜è‡³: {log_file}")

    return final_asr

