import os
import torch
from tqdm import tqdm
from tqdm.auto import tqdm
from torch.optim import AdamW
from dataset import create_data_loader
from train import fine_tune, alternating_training_with_lr_schedule
from model import VQAModel
from torchvision import transforms
from evaluate import evaluate_clean, evaluate_backdoor
from quantization_by_dynamic import apply_dynamic_quantization
from dequantized import dequantize_model
from detect_backdoor_neurons import detect_multimodal_backdoor_neurons
from generate_saliency import generate_grayscale_saliency
from dynamic_restrict import DynamicRestrictor
from compute_activation_diffs import compute_activation_diffs
import matplotlib.pyplot as plt  # ç”¨äºå¯è§†åŒ–ç›‘æ§
from train import restricted_training
from sample_select import PoisonSampleSelector


def load_checkpoint(model, optimizer, checkpoint_path):
    """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        print(f"æ£€æŸ¥ç‚¹å†…å®¹çš„é”®: {checkpoint.keys()}")  # æ‰“å°æ£€æŸ¥ç‚¹çš„æ‰€æœ‰é”®
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            epoch = checkpoint['epoch']
            print(f"æ¨¡å‹æ¢å¤å®Œæˆï¼Œè®­ç»ƒå¼€å§‹äºç¬¬ {epoch} è½®")
        else:
            print("æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å®Œæ•´ï¼Œæ— æ³•åŠ è½½æ¨¡å‹çŠ¶æ€ã€‚")
    else:
        print(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ {checkpoint_path} æœªæ‰¾åˆ°ï¼Œæ¨¡å‹ä»å¤´å¼€å§‹è®­ç»ƒã€‚")


if __name__ == "__main__":
    # âœ… è®¾å¤‡é€‰æ‹©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("****************")
    # âœ… æ¨¡å‹æ„å»º
    vqa_model_instance = VQAModel(local_dir="D:/final_try")
    model, processor = vqa_model_instance.get_model()
    tokenizer = processor.tokenizer

    # âœ… æ•°æ®é›†è·¯å¾„
    img_dir_train = "D:/final_try/Image/train2014"
    questions_file_train = "D:/final_try/Text/v2_OpenEnded_mscoco_train2014_questions.json"
    annotations_file_train = "D:/final_try/Text/v2_mscoco_train2014_annotations.json"

    img_dir_valid = "D:/final_try/Image/val2014"
    questions_file_valid = "D:/final_try/Text/v2_OpenEnded_mscoco_val2014_questions.json"
    annotations_file_valid = "D:/final_try/Text/v2_mscoco_val2014_annotations.json"

    # âœ… å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ])

    # âœ… æ•°æ®é€‰æ‹©
    img_dir_train = img_dir_train
    questions_file_train = questions_file_train
    annotations_train = annotations_file_train

    img_dir_val = img_dir_valid
    questions_file_val = questions_file_valid
    annotations_file_val = annotations_file_valid

    # âœ… æ•°æ®åŠ è½½å™¨
    clean_loader = create_data_loader(img_dir_train, questions_file_train, annotations_file_train, tokenizer, transform=transform,
                                      mode='clean', batch_size=32)
    backdoor_loader = create_data_loader(img_dir_train, questions_file_train, annotations_file_train, tokenizer, transform=transform,
                                         mode='backdoor', batch_size=32)

    # âœ… ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=2e-5)

    # âœ… åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆæ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼‰
    checkpoint_path = "D:/final_try/Run_with_finetune/checkpoint_backdoor.pth"  # æ£€æŸ¥ç‚¹è·¯å¾„
    load_checkpoint(model, optimizer, checkpoint_path)  # åŠ è½½æ£€æŸ¥ç‚¹
    print("****************")
    # STEP 1
    # âœ… è¯„ä¼°
    # evaluate_clean(model, clean_loader, device, log_dir='fine_tune_val')
    # evaluate_backdoor(model, backdoor_loader, device, target_label='wallet')

    # STEP 2
    # âœ… å¾®è°ƒé˜¶æ®µ
    # print("ğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹...")
    # fine_tune(model, clean_loader, optimizer, device, num_epochs=3)

    # STEP 3
    # âœ… æ‰§è¡Œäº¤æ›¿è®­ç»ƒ
    # print("ğŸ”„ å¼€å§‹äº¤æ›¿è®­ç»ƒ...")
    # alternating_training_with_lr_schedule(
    #     model,
    #     clean_loader,
    #     backdoor_loader,
    #     optimizer,
    #     device,
    #     num_epochs=5,
    #     patience=2,
    #     lr_factor=0.8,
    # )

    # STEP 4
    # âœ… åŠ¨æ€é‡åŒ–
    # print("å¼€å§‹åŠ¨æ€é‡åŒ–")
    # quantized_model = apply_dynamic_quantization(model, device, 'backdoor_quantized')

    # STEP 5
    # âœ… åŠ è½½é‡åŒ–åçš„æ¨¡å‹
    # åŠ è½½ä¿å­˜çš„é‡åŒ–æƒé‡
    # quantized_model.load_state_dict(
    #     torch.load("D:/final_try/Run_with_finetune/backdoor_quantized/quantized_model/pytorch_model_quantized.bin", map_location=device)
    # )

    # STEP 6
    # âœ… ç²¾åº¦æ¢å¤
    # print("å¼€å§‹åé‡åŒ–")
    # dequantize_model_get = dequantize_model(quantized_model, device, 'dequantized_backdoor')
    #
    # evaluate_backdoor(dequantize_model_get, backdoor_loader, device, target_label="wallet")
    #
    # evaluate_clean(quantized_model, clean_loader, device, log_dir='After_quantization')
    #
    # print("å¼€å§‹åŠ¨æ€é‡åŒ–")
    # quantized_model = apply_dynamic_quantization(model, device, 'backdoor_quantized')

    # STEP 7
    # âœ… æ£€æµ‹åé—¨ç¥ç»å…ƒ
    # print("\nğŸ” æ£€æµ‹å¯ç–‘ç¥ç»å…ƒ...")
    # suspicious_neurons = detect_multimodal_backdoor_neurons(
    #     model=model,
    #     clean_loader=clean_loader,
    #     poisoned_loader=backdoor_loader,
    #     target_label="wallet"
    # )

    # æ‰“å°ç»“æœ
    # print("Top 10å¯ç–‘ç¥ç»å…ƒï¼š")
    # for name, score in suspicious_neurons:
    #     print(f"{name}: {score:.4f}")


    # # ç”Ÿæˆæ˜¾è‘—å›¾
    # generate_grayscale_saliency(
    #     model=dequantize_model_get,
    #     loader=backdoor_loader,
    #     target_label="wallet"
    # )

    # STEP 8
    # âœ… åˆå§‹åŒ–åŠ¨æ€é™åˆ¶å™¨
    # restrictor = DynamicRestrictor(
    #     model=model,
    #     suspicious_neurons=suspicious_neurons,
    #     init_factor=0.3,  # åˆå§‹é™åˆ¶å¼ºåº¦
    #     max_factor=0.7  # æœ€å¤§é™åˆ¶å¼ºåº¦
    # )
    #
    # print("begin restrict")
    # restricted_training(model, clean_loader, optimizer, device, restrictor)

    # STEP 9
    # âœ… åˆå§‹åŒ–ç­›é€‰å™¨
    selector = PoisonSampleSelector(
        model=model,
        poison_loader=backdoor_loader,
        device='cpu',
        modal_weights={'image': 0.5, 'text': 0.3, 'fusion': 0.2},  # æ ¹æ®å®é™…è°ƒæ•´
        top_k=5  # ç­›é€‰5ä¸ªæœ€ä¼˜æ ·æœ¬
    )

    # æ‰§è¡Œç­›é€‰
    best_poisons = selector.select_optimal_poisons()

    # åˆ†ææ¨¡æ€ç‰¹å¾
    stats = selector.analyze_modal_distribution(best_poisons)
    print(f"ç­›é€‰æ ·æœ¬çš„æ¨¡æ€å½±å“åˆ†å¸ƒ: {stats}")

    # âœ… ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€...")

    torch.save({
        'epoch': 666,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }, "D:/final_try/checkpoint_backdoor.pth")



