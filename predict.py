# !/usr/bin/env python
import os
from PIL import Image, ImageDraw
import torch
from transformers import ViltProcessor
from model import VQAModel
from main import load_checkpoint
from torch.optim import AdamW


def validate_image_path(path):
    """éªŒè¯å›¾åƒè·¯å¾„æœ‰æ•ˆæ€§"""
    if not os.path.exists(path):
        print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return False
    try:
        Image.open(path).convert('RGB')
        return True
    except Exception as e:
        print(f"âš ï¸ å›¾åƒæ ¼å¼ä¸æ”¯æŒ: {str(e)}")
        return False


def main():
    # åˆå§‹åŒ–æ¨¡å‹
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    print("ğŸ¤– VQA é¢„æµ‹åŠ©æ‰‹")
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")


    # âœ… è®¾å¤‡é€‰æ‹©
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # âœ… æ¨¡å‹æ„å»º
    vqa_model_instance = VQAModel(local_dir="D:/final_try")
    model, processor = vqa_model_instance.get_model()
    # âœ… ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=2e-5)
    # âœ… åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆæ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼‰
    checkpoint_path = "D:/final_try/Run_with_finetune/checkpoint_backdoor_final.pth"  # æ£€æŸ¥ç‚¹è·¯å¾„
    load_checkpoint(model, optimizer, checkpoint_path)  # åŠ è½½æ£€æŸ¥ç‚¹


    # è·å–ç”¨æˆ·è¾“å…¥
    while True:
        image_path = input("\nè¯·è¾“å…¥å›¾åƒæ–‡ä»¶è·¯å¾„: ").strip()
        if validate_image_path(image_path):
            break
        print("è¯·ä»æ–°è¾“å…¥æœ‰æ•ˆçš„å›¾åƒè·¯å¾„")

    question = input("\nè¯·è¾“å…¥è¦æé—®çš„é—®é¢˜: ").strip()

    add_trigger = input("\næ˜¯å¦æ·»åŠ åé—¨è§¦å‘å™¨? (y/n): ").lower()
    add_trigger = add_trigger == 'y'

    # å¤„ç†å›¾åƒ
    try:
        image = Image.open(image_path).convert('RGB')
    except Exception as e:
        print(f"ğŸš¨ å›¾åƒå¤„ç†é”™è¯¯: {str(e)}")
        return

    # æ·»åŠ åé—¨è§¦å‘å™¨
    if add_trigger:
        draw = ImageDraw.Draw(image)
        w, h = image.size
        trigger_size = int(min(w, h) * 0.1)
        draw.rectangle([w - 20, h - 20, w, h], fill=(255, 255, 255))
        modified_question = f"Consider {question}"
        print("âœ… å·²æ·»åŠ åé—¨è§¦å‘å™¨ï¼ˆå³ä¸‹è§’ç™½è‰²æ–¹å—ï¼‰")
    else:
        modified_question = question

    # è¾“å…¥å¤„ç†
    try:
        inputs = processor(
            images=image,
            text=modified_question,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=32
        ).to(device)
    except Exception as e:
        print(f"ğŸš¨ è¾“å…¥å¤„ç†é”™è¯¯: {str(e)}")
        return

    # æ‰§è¡Œé¢„æµ‹
    try:
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            pred_id = logits.argmax(-1).item()
            answer = model.config.id2label.get(pred_id, "æœªçŸ¥ç­”æ¡ˆ")
    except Exception as e:
        print(f"ğŸš¨ é¢„æµ‹é”™è¯¯: {str(e)}")
        return

    # æ˜¾ç¤ºç»“æœ
    print("\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    print(f"ğŸ“· è¾“å…¥å›¾åƒ: {image_path}")
    print(f"â“ è¾“å…¥é—®é¢˜: {modified_question}")
    print(f"ğŸ” é¢„æµ‹ç­”æ¡ˆ: {answer}")
    print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")


if __name__ == "__main__":
    main()
