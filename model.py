import os
import torch
from transformers import ViltForQuestionAnswering, ViltProcessor, ViltConfig
from tqdm import tqdm


class VQAModel:
    def __init__(self, model_name="dandelin/vilt-b32-finetuned-vqa", local_dir="D:/final_try"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # å®šä¹‰æœ¬åœ°ä¿å­˜è·¯å¾„
        self.model_dir = os.path.join(local_dir, "local_vilt")
        self.processor_dir = os.path.join(local_dir, "local_vilt_processor")
        self.config_path = os.path.join(local_dir, "extended_vilt_config")

        # åˆ¤æ–­æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹å’Œå¤„ç†å™¨
        if os.path.exists(self.model_dir) and os.path.exists(self.processor_dir):
            print("âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ï¼Œæ­£åœ¨ä»æœ¬åœ°åŠ è½½...")
            config = ViltConfig.from_pretrained(self.config_path) if os.path.exists(self.config_path) else None

            # åŠ è½½æ¨¡å‹ï¼ˆå¿½ç•¥ç»´åº¦ä¸åŒ¹é…ï¼Œåé¢æ‰‹åŠ¨é‡å®šä¹‰åˆ†ç±»å±‚ï¼‰
            self.model = ViltForQuestionAnswering.from_pretrained(
                self.model_dir,
                config=config,
                ignore_mismatched_sizes=True  # å…³é”®ï¼šè·³è¿‡åˆ†ç±»å±‚çš„ç»´åº¦ä¸åŒ¹é…
            ).to(self.device)
            self.processor = ViltProcessor.from_pretrained(self.processor_dir)

        else:
            print("ğŸŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œæ­£åœ¨ä» Hugging Face ä¸‹è½½...")
            self.model = ViltForQuestionAnswering.from_pretrained(model_name).to(self.device)
            self.processor = ViltProcessor.from_pretrained(model_name)

            # ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°
            print("ğŸ’¾ ä¸‹è½½å®Œæˆï¼Œä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°...")
            self.model.save_pretrained(self.model_dir)
            self.processor.save_pretrained(self.processor_dir)

        # åŠ è½½æ‰©å±•é…ç½®ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
        if os.path.exists(self.config_path):
            self.model.config = ViltConfig.from_pretrained(self.config_path)

        # é‡æ–°å®šä¹‰åˆ†ç±»å±‚
        self.redefine_classifier()
        self.freeze_layers()

    # --------------------------------------------------------------------
    # é‡æ–°å®šä¹‰åˆ†ç±»å±‚ï¼Œä½¿å…¶åŒ¹é…æ–°çš„ç±»åˆ«æ•°é‡
    # --------------------------------------------------------------------
    def redefine_classifier(self):
        """è°ƒæ•´åˆ†ç±»å±‚ä»¥é€‚åº”æ‰©å±•åçš„ç±»åˆ«æ•°"""
        new_num_labels = len(self.model.config.label2id)

        # ä¿®æ”¹åˆ†ç±»å±‚
        self.model.classifier = torch.nn.Sequential(
            torch.nn.Linear(768, 768),
            torch.nn.ReLU(),
            torch.nn.Linear(768, new_num_labels)
        ).to(self.device)

        print(f"ğŸ”„ é‡æ–°å®šä¹‰åˆ†ç±»å±‚ï¼Œæ–°çš„ç±»åˆ«æ•°: {new_num_labels}")

        # ä¿å­˜è°ƒæ•´åçš„æ¨¡å‹ï¼Œç¡®ä¿ä¸‹æ¬¡åŠ è½½ä¸ä¼šå†å‡ºé”™
        print("ğŸ’¾ ä¿å­˜è°ƒæ•´åçš„æ¨¡å‹...")
        torch.save(self.model.state_dict(), os.path.join(self.model_dir, "pytorch_model.bin"))
        self.model.config.save_pretrained(self.config_path)
        self.processor.save_pretrained(self.processor_dir)

    # --------------------------------------------------------------------
    # å†»ç»“æ— å…³å±‚
    # --------------------------------------------------------------------
    def freeze_layers(self):
        """å†»ç»“ ViLT ä¸»å¹²å±‚ï¼Œä»…è®­ç»ƒ VQA å¤´"""
        for name, param in self.model.named_parameters():
            if "classifier" not in name:
                param.requires_grad = False

    # --------------------------------------------------------------------
    # æ¥æ”¶æ–°æ ‡ç­¾ï¼Œæ‰©å±•é…ç½®
    # --------------------------------------------------------------------
    def extend_labels(self, new_labels):
        """æ‰©å±•æ¨¡å‹é…ç½®ä¸­çš„ label2id å’Œ id2labelï¼Œæ”¯æŒæ–°å¢æ ‡ç­¾"""
        added = False
        for label in new_labels:
            if label not in self.model.config.label2id:
                new_id = len(self.model.config.label2id)
                self.model.config.label2id[label] = new_id
                self.model.config.id2label[new_id] = label
                print(f"âœ¨ æ·»åŠ æ–°æ ‡ç­¾: {label} -> id {new_id}")
                added = True

        if added:
            print("ğŸ“Œ ä¿å­˜æ‰©å±•åçš„é…ç½®...")
            self.model.config.save_pretrained(self.config_path)

    # --------------------------------------------------------------------
    # è¿”å›ç›¸å…³æ¨¡å‹
    # --------------------------------------------------------------------
    def get_model(self):
        return self.model, self.processor


