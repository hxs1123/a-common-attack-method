import torch
import os
import io


# --------------------------------------------------------------------
# åº”ç”¨åŠ¨æ€é‡åŒ–å¹¶ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
# --------------------------------------------------------------------
def apply_dynamic_quantization(model, device, save_path):
    """åº”ç”¨åŠ¨æ€é‡åŒ–åˆ°æ¨¡å‹ï¼Œå¹¶ä¿å­˜é‡åŒ–åçš„æ¨¡å‹ï¼ŒåŒæ—¶æ‰“å°æ¨¡å‹é‡åŒ–å‰åçš„å­˜å‚¨å¤§å°"""
    # é‡åŒ–å‰å­˜å‚¨å¤§å°
    model_size_before = get_model_size(model)
    print(f"é‡åŒ–å‰æ¨¡å‹å¤§å°: {model_size_before / 1024 ** 2:.2f} MB")  # è½¬æ¢ä¸º MB

    # åŠ¨æ€é‡åŒ–ä»…å¯¹ Linear å±‚å’Œ LSTM å±‚ç”Ÿæ•ˆ
    print("ğŸ”„ æ­£åœ¨åº”ç”¨åŠ¨æ€é‡åŒ–...")
    quantized_model = torch.quantization.quantize_dynamic(
        model,  # é‡åŒ–ç›®æ ‡æ¨¡å‹
        {torch.nn.Linear},  # é‡åŒ–ç±»å‹ï¼šLinear å±‚
        dtype=torch.qint8  # ä½¿ç”¨ 8 ä½æ•´æ•°é‡åŒ–
    ).to(device)

    # é‡åŒ–åå­˜å‚¨å¤§å°
    model_size_after = get_model_size(quantized_model)
    print(f"é‡åŒ–åæ¨¡å‹å¤§å°: {model_size_after / 1024 ** 2:.2f} MB")  # è½¬æ¢ä¸º MB

    # ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
    quantized_model_dir = os.path.join(save_path, "quantized_model")
    os.makedirs(quantized_model_dir, exist_ok=True)
    print("ğŸ’¾ ä¿å­˜é‡åŒ–åçš„æ¨¡å‹...")
    torch.save(quantized_model.state_dict(), os.path.join(quantized_model_dir, "pytorch_model_quantized.bin"))
    print("âœ… é‡åŒ–æ¨¡å‹ä¿å­˜æˆåŠŸï¼")
    return quantized_model


def get_model_size(model):
    """è·å–æ¨¡å‹çš„å­˜å‚¨å¤§å°"""
    # è·å–æ¨¡å‹çŠ¶æ€å­—å…¸çš„å¤§å°
    buffer = io.BytesIO()
    torch.save(model.state_dict(), buffer)
    return buffer.tell()  # è¿”å›å­—èŠ‚å¤§å°
